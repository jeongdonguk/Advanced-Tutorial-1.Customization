{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMXwQCcUVQylCYfSeJrTh+W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"1LI4Ly7b-eQl","executionInfo":{"status":"ok","timestamp":1674911187130,"user_tz":-540,"elapsed":235,"user":{"displayName":"화정이네","userId":"05305307080143119231"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","source":["### Tensor\n","텐서는 다차원 배열입니다. numpy의 배열과 유사하게 tf.Tensor객체에는 데이터 유형과 모양이 있습니다. tf.Tensor는 GPU와 같은 가속기 메모리에 상주할 수 있습니다. Tensorflow는 tf.Tensor를 소비하고 생성하는 풍부한 연산 라이브러리를 제공합니다.(tf.math.add(덧셈), tf.linalg.matmul(행렬곱), tf.linalg.inv(역행렬) 등). 이러한 연산은 기본 python 유형을 자동으로 변환합니다."],"metadata":{"id":"8hZdRgGcCfUK"}},{"cell_type":"code","source":["print(tf.math.add(1,2))\n","print(tf.math.add([1,2],[3,4]))\n","print(tf.math.square(5))\n","print(tf.math.reduce_sum([1,2,3]))\n","print(tf.math.reduce_sum([[1,2,3],[1,2,5]]))\n","print(tf.math.square(2) + tf.math.square(3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UM_aEU1oDZmP","executionInfo":{"status":"ok","timestamp":1674911190463,"user_tz":-540,"elapsed":388,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"da2cc1d6-698a-491a-98e4-f09fddf01e08"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(3, shape=(), dtype=int32)\n","tf.Tensor([4 6], shape=(2,), dtype=int32)\n","tf.Tensor(25, shape=(), dtype=int32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(14, shape=(), dtype=int32)\n","tf.Tensor(13, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["x = tf.linalg.matmul([[2]],[[2,3]])\n","print(x)\n","print(x.shape)\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cio6QnHyEuvz","executionInfo":{"status":"ok","timestamp":1674911190728,"user_tz":-540,"elapsed":2,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"c9442a1a-4748-4a76-b228-2a4f991b1a2e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[4 6]], shape=(1, 2), dtype=int32)\n","(1, 2)\n","<dtype: 'int32'>\n"]}]},{"cell_type":"markdown","source":["### tf.Tensor의 장점\n"," - 텐서플로 연산은 자동으로 numpy배열을 텐서로 변환합니다.\n"," - numpy연산은 자동으로 텐서를 numpy배열로 변환합니다.\n"," - 텐서는 .numpy() 메서드를 통해 numpy배열로 변환이 가능합니다. 그러나 tf.Tensor는 GPU 메모리에 저장될 수 있고, numpy 배열은 항상 호스트 메모리에 저장되므로, 이러한 변환이 항상 가능한 것은 아닙니다. 따라서 gpu에서 호스트 메모리로 복사가 필요합니다."],"metadata":{"id":"_2yJxNiCMH4g"}},{"cell_type":"code","source":["import numpy as np\n","\n","ndarray = np.ones([3,3])\n","print('ndarray : \\n',ndarray)\n","tensor = tf.math.multiply(ndarray, 42)\n","print('tensor : \\n',tensor)\n","# 자동으로 텐서가 넘파이 배열화 되어 연산됨을 확인해본다.\n","print('\\n',np.add(tensor,1))\n","# 매서드를 통한 넘파이와 텐서간의 변화를 확인해본다.\n","print('\\n',tensor.numpy())"],"metadata":{"id":"7hZyiATDPDqN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674911399277,"user_tz":-540,"elapsed":237,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"d636adb7-2b14-4a5c-dff1-54d756e5fe03"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["ndarray : \n"," [[1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]]\n","tensor : \n"," tf.Tensor(\n","[[42. 42. 42.]\n"," [42. 42. 42.]\n"," [42. 42. 42.]], shape=(3, 3), dtype=float64)\n","\n"," [[43. 43. 43.]\n"," [43. 43. 43.]\n"," [43. 43. 43.]]\n","\n"," [[42. 42. 42.]\n"," [42. 42. 42.]\n"," [42. 42. 42.]]\n"]}]},{"cell_type":"markdown","source":["## GPU 가속\n"," 대부분의 텐서 연산은 GPU를 사용하여 가속한다. 따로 코드를 명시하지 않아도 tensorflow는 연산을 위해 CPU or GPU의 사용을 자동으로 결정한다. 필요시 둘 사이의 메모리에서 복사되기도 합니다. 연산에 의해 생성된 텐서는 연산이 실행된 장치의 메모리에 의해 실행됩니다.\n"," <br>예시는 아래와 같습니다"],"metadata":{"id":"gOiPGw6pfI46"}},{"cell_type":"code","source":["x = tf.random.uniform([3,3])\n","print(x)\n","\n","print('GPU장치가 있는지 확인해본다.')\n","print(tf.config.list_physical_devices())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQvKQbdqhNxM","executionInfo":{"status":"ok","timestamp":1674912361325,"user_tz":-540,"elapsed":1,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"40ed1c6a-44f7-4b18-9d0d-59eb914e7432"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[0.98220205 0.75512254 0.70742786]\n"," [0.07006562 0.38545406 0.47248185]\n"," [0.16607213 0.7613646  0.6220701 ]], shape=(3, 3), dtype=float32)\n","GPU장치가 있는지 확인해본다.\n","[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"]}]},{"cell_type":"markdown","source":["### 위의 방식으로 확인해보면 GPU가 없음을 알 수 있다. 필자는 구글의 코랩 유료버전을 사용중이므로 TPU를 이용할 것이며 TPU에 대한 확인과 사용방법은 아래와 같다."],"metadata":{"id":"badhZLZehrLj"}},{"cell_type":"code","source":["import os\n","x = tf.random.uniform([3,3])\n","print(x)\n","\n","print('TPU장치가 있는지 확인해본다.')\n","print(os.environ['COLAB_TPU_ADDR'])\n","# tpu이름을 통해 해당 장치를 지정하여 사용이 가능한데 나의 tpu이름은 아래와 같이 사용하면 된다.\n","tpu_name = 'grpc://'+os.environ['COLAB_TPU_ADDR']\n","print('tpu_name : ',tpu_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Mje8zRqk65q","executionInfo":{"status":"ok","timestamp":1674912910173,"user_tz":-540,"elapsed":3,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"bc718f4d-eeb5-4acf-defd-aef469ff02f7"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[0.31053042 0.52364016 0.25591218]\n"," [0.02147949 0.14401782 0.20363021]\n"," [0.4052142  0.75381637 0.71540976]], shape=(3, 3), dtype=float32)\n","TPU장치가 있는지 확인해본다.\n","10.1.19.202:8470\n","tpu_name :  grpc://10.1.19.202:8470\n"]}]},{"cell_type":"markdown","source":["TPU와 CPU의 성능 차이를 비교해 보겠다."],"metadata":{"id":"FDf1fZzblHxJ"}},{"cell_type":"code","source":["# tpu 사용준비"],"metadata":{"id":"J1sZ8xTcnPdM"},"execution_count":null,"outputs":[]}]}