{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNtZ9PR7jDfK00tDOi6x0Ko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1LI4Ly7b-eQl","executionInfo":{"status":"ok","timestamp":1674915607173,"user_tz":-540,"elapsed":2846,"user":{"displayName":"화정이네","userId":"05305307080143119231"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","source":["### Tensor\n","텐서는 다차원 배열입니다. numpy의 배열과 유사하게 tf.Tensor객체에는 데이터 유형과 모양이 있습니다. tf.Tensor는 GPU와 같은 가속기 메모리에 상주할 수 있습니다. Tensorflow는 tf.Tensor를 소비하고 생성하는 풍부한 연산 라이브러리를 제공합니다.(tf.math.add(덧셈), tf.linalg.matmul(행렬곱), tf.linalg.inv(역행렬) 등). 이러한 연산은 기본 python 유형을 자동으로 변환합니다."],"metadata":{"id":"8hZdRgGcCfUK"}},{"cell_type":"code","source":["print(tf.math.add(1,2))\n","print(tf.math.add([1,2],[3,4]))\n","print(tf.math.square(5))\n","print(tf.math.reduce_sum([1,2,3]))\n","print(tf.math.reduce_sum([[1,2,3],[1,2,5]]))\n","print(tf.math.square(2) + tf.math.square(3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UM_aEU1oDZmP","executionInfo":{"status":"ok","timestamp":1674915607614,"user_tz":-540,"elapsed":443,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"d9e55846-94a3-4577-c181-8e26e8d26a38"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(3, shape=(), dtype=int32)\n","tf.Tensor([4 6], shape=(2,), dtype=int32)\n","tf.Tensor(25, shape=(), dtype=int32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(14, shape=(), dtype=int32)\n","tf.Tensor(13, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["x = tf.linalg.matmul([[2]],[[2,3]])\n","print(x)\n","print(x.shape)\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cio6QnHyEuvz","executionInfo":{"status":"ok","timestamp":1674915607615,"user_tz":-540,"elapsed":5,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"4a589595-8f63-4f2c-e2b8-d14566483570"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[4 6]], shape=(1, 2), dtype=int32)\n","(1, 2)\n","<dtype: 'int32'>\n"]}]},{"cell_type":"markdown","source":["### tf.Tensor의 장점\n"," - 텐서플로 연산은 자동으로 numpy배열을 텐서로 변환합니다.\n"," - numpy연산은 자동으로 텐서를 numpy배열로 변환합니다.\n"," - 텐서는 .numpy() 메서드를 통해 numpy배열로 변환이 가능합니다. 그러나 tf.Tensor는 GPU 메모리에 저장될 수 있고, numpy 배열은 항상 호스트 메모리에 저장되므로, 이러한 변환이 항상 가능한 것은 아닙니다. 따라서 gpu에서 호스트 메모리로 복사가 필요합니다."],"metadata":{"id":"_2yJxNiCMH4g"}},{"cell_type":"code","source":["import numpy as np\n","\n","ndarray = np.ones([3,3])\n","print('ndarray : \\n',ndarray)\n","tensor = tf.math.multiply(ndarray, 42)\n","print('tensor : \\n',tensor)\n","# 자동으로 텐서가 넘파이 배열화 되어 연산됨을 확인해본다.\n","print('\\n',np.add(tensor,1))\n","# 매서드를 통한 넘파이와 텐서간의 변화를 확인해본다.\n","print('\\n',tensor.numpy())"],"metadata":{"id":"7hZyiATDPDqN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674915607615,"user_tz":-540,"elapsed":4,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"82e1a9ef-b5ae-4807-8694-f710d3bf4d5e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ndarray : \n"," [[1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]]\n","tensor : \n"," tf.Tensor(\n","[[42. 42. 42.]\n"," [42. 42. 42.]\n"," [42. 42. 42.]], shape=(3, 3), dtype=float64)\n","\n"," [[43. 43. 43.]\n"," [43. 43. 43.]\n"," [43. 43. 43.]]\n","\n"," [[42. 42. 42.]\n"," [42. 42. 42.]\n"," [42. 42. 42.]]\n"]}]},{"cell_type":"markdown","source":["## GPU 가속\n"," 대부분의 텐서 연산은 GPU를 사용하여 가속한다. 따로 코드를 명시하지 않아도 tensorflow는 연산을 위해 CPU or GPU의 사용을 자동으로 결정한다. 필요시 둘 사이의 메모리에서 복사되기도 합니다. 연산에 의해 생성된 텐서는 연산이 실행된 장치의 메모리에 의해 실행됩니다.\n"," <br>예시는 아래와 같습니다"],"metadata":{"id":"gOiPGw6pfI46"}},{"cell_type":"code","source":["x = tf.random.uniform([3,3])\n","print(x)\n","\n","print('GPU장치가 있는지 확인해본다.')\n","print(tf.config.list_physical_devices())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQvKQbdqhNxM","executionInfo":{"status":"ok","timestamp":1674915607616,"user_tz":-540,"elapsed":4,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"4cd8e874-c53a-4392-b655-01770e557aac"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[0.12367165 0.35694635 0.618688  ]\n"," [0.80290425 0.26262748 0.6492804 ]\n"," [0.72215164 0.09392834 0.2733034 ]], shape=(3, 3), dtype=float32)\n","GPU장치가 있는지 확인해본다.\n","[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"]}]},{"cell_type":"markdown","source":["### 위의 방식으로 확인해보면 GPU가 없음을 알 수 있다. 필자는 구글의 코랩 유료버전을 사용중이므로 TPU를 이용할 것이며 TPU에 대한 확인과 사용방법은 아래와 같다."],"metadata":{"id":"badhZLZehrLj"}},{"cell_type":"code","source":["import os\n","x = tf.random.uniform([3,3])\n","print(x)\n","# 현재 검색가능한 디바이스 검색\n","print(tf.config.list_physical_devices())\n","print('TPU장치가 있는지 확인해본다.')\n","print(os.environ['COLAB_TPU_ADDR'])\n","# tpu이름을 통해 해당 장치를 지정하여 사용이 가능한데 나의 tpu이름은 아래와 같이 사용하면 된다.\n","tpu_name = 'grpc://'+os.environ['COLAB_TPU_ADDR']\n","print('tpu_name : ',tpu_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Mje8zRqk65q","executionInfo":{"status":"ok","timestamp":1674915611252,"user_tz":-540,"elapsed":3,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"b2c22778-4431-4ca5-bce3-5fc229e137cf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[0.64974487 0.24309683 0.0386498 ]\n"," [0.79751897 0.59210503 0.4541067 ]\n"," [0.00106108 0.7817106  0.78778374]], shape=(3, 3), dtype=float32)\n","[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n","TPU장치가 있는지 확인해본다.\n","10.88.69.138:8470\n","tpu_name :  grpc://10.88.69.138:8470\n"]}]},{"cell_type":"markdown","source":["TPU와 CPU의 성능 차이를 비교해 보겠다.<br>\n","CPU 사용법"],"metadata":{"id":"FDf1fZzblHxJ"}},{"cell_type":"code","source":["%%time\n","print('on cpu:')\n","with tf.device('CPU:0'):\n","    x = tf.random.uniform([1000, 1000])\n","    assert x.device.endswith('CPU:0')\n","    for i in range(100):\n","        tf.linalg.matmul(x,x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xabg7nNOv8d8","executionInfo":{"status":"ok","timestamp":1674915618928,"user_tz":-540,"elapsed":267,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"97e1d727-07d7-4559-e7e7-7d07900ac863"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["on cpu:\n","CPU times: user 6.58 s, sys: 65.2 ms, total: 6.64 s\n","Wall time: 199 ms\n"]}]},{"cell_type":"markdown","source":["### TPU사용법"],"metadata":{"id":"VJGSKuiTw21g"}},{"cell_type":"code","source":["# tpu 사용준비\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\n","\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)"],"metadata":{"id":"J1sZ8xTcnPdM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674915845091,"user_tz":-540,"elapsed":13726,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"b47c82c6-cff8-4e32-d22f-f110e1f41eeb"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.tpu.topology.Topology at 0x7f1fd6626580>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["strategy = tf.distribute.TPUStrategy(resolver)"],"metadata":{"id":"y2_9Uku3rDfv","executionInfo":{"status":"ok","timestamp":1674915855532,"user_tz":-540,"elapsed":250,"user":{"displayName":"화정이네","userId":"05305307080143119231"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### 텐서플로우의 경우 자동적으로 장치를 할당하지만 아래와 같이 명시적으로 사용하여 장치를 사용할 수 있습니다."],"metadata":{"id":"rh-IbjzQuadK"}},{"cell_type":"code","source":["%%time\n","print('on TPU:')\n","with strategy.scope():\n","    x = tf.random.uniform([1000, 1000])\n","    for i in range(100):\n","        tf.linalg.matmul(x,x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJLbPlbHvXKQ","executionInfo":{"status":"ok","timestamp":1674915881392,"user_tz":-540,"elapsed":238,"user":{"displayName":"화정이네","userId":"05305307080143119231"}},"outputId":"d6e6d46f-4ede-45eb-b254-cc84c20b49a8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["on TPU:\n","CPU times: user 10.9 ms, sys: 7.47 ms, total: 18.4 ms\n","Wall time: 15.7 ms\n"]}]},{"cell_type":"markdown","source":["#### 위를 보면 알 수 있듯이 실제 실행시간이(wall time) tpu를 사용한 경우 많이 줄어들었음을 알 수 있다."],"metadata":{"id":"m2nB12NuvdQz"}},{"cell_type":"code","source":[],"metadata":{"id":"k-RrTu5bxOXU"},"execution_count":null,"outputs":[]}]}